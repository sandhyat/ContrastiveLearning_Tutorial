{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a9ceef",
   "metadata": {},
   "source": [
    "# SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8645cf5",
   "metadata": {},
   "source": [
    "![image](SCARF_Schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b1560",
   "metadata": {},
   "source": [
    "This file contains an implementation of SCARF on a publicly available [Darwin dataset](https://archive.ics.uci.edu/dataset/732/darwin). \n",
    "The dataset contains 451 features across 174 participants with 2 classes: Alzheimer's disease patients or healthy.\n",
    "The goal is to  learn a tabular data representation in an unsupervised or self-supervised such that the learnt representations can be good at the classification task.\n",
    "\n",
    "Adapted from [pytorch implementation of SCARF](https://github.com/clabrugere/pytorch-scarf/tree/master)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cde05a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay, classification_report,\n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "from scarf import scarf_model, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af320206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is :  /home/trips/ContrastiveLearning_Tutorial/scarf\n"
     ]
    }
   ],
   "source": [
    "# check the current directory, set it as scarf\n",
    "if os.getcwd()!= '/home/trips/ContrastiveLearning_Tutorial/scarf':  # replace the path with your local path\n",
    "    os.chdir('/home/trips/ContrastiveLearning_Tutorial/scarf')\n",
    "print(\"Current working directory is : \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe95cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for initial loading and processing of the dataset\n",
    "def load_dataset(dataset):\n",
    "    data_file = os.path.join('../Small_datasets', dataset, dataset + \".csv\")\n",
    "    df = pd.read_csv(data_file)\n",
    "    \n",
    "    # convert the target into numeric\n",
    "    df.loc[df['class']=='P','class']=1\n",
    "    df.loc[df['class']=='H','class']=0\n",
    "    \n",
    "    data, target = df.iloc[:,1:], df.iloc[:,-1]  # first column is the id and last column is the target\n",
    "\n",
    "\n",
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        data, \n",
    "        target, \n",
    "        test_size=0.2, \n",
    "        stratify=target, \n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    # preprocess\n",
    "    constant_cols = [c for c in train_data.columns if train_data[c].nunique() == 1]\n",
    "    train_data.drop(columns=constant_cols, inplace=True)\n",
    "    test_data.drop(columns=constant_cols, inplace=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_data = pd.DataFrame(scaler.fit_transform(train_data), columns=train_data.columns)\n",
    "    test_data = pd.DataFrame(scaler.transform(test_data), columns=test_data.columns)\n",
    "\n",
    "    # to torch dataset\n",
    "    train_ds = scarf_model.ExampleDataset(\n",
    "        train_data.to_numpy(), \n",
    "        train_target.to_numpy(), \n",
    "        columns=train_data.columns\n",
    "    )\n",
    "    test_ds = scarf_model.ExampleDataset(\n",
    "        test_data.to_numpy(), \n",
    "        test_data.to_numpy(), \n",
    "        columns=test_data.columns\n",
    "    )\n",
    "\n",
    "    print(f\"Train set: {train_ds.shape}\")\n",
    "    print(f\"Test set: {test_ds.shape}\")\n",
    "    train_ds.to_dataframe().head()\n",
    "\n",
    "    return train_ds, train_target, test_ds, test_target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab6debcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setting and the dataset choice\n",
    "\n",
    "dataset = \"Darwin\" # dataset name\n",
    "batch_size=128\n",
    "epochs = 100\n",
    "lr=0.001  # learning rate\n",
    "repr_dims=16  # representation dimension\n",
    "seed = 100  # random seed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "corruptionRate = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4924843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (139, 451)\n",
      "Test set: (35, 451)\n",
      " Training data size :   (139, 451)\n",
      " Test data size :   (35, 451)\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "train_data, train_labels, test_data, test_labels = load_dataset(dataset)\n",
    "\n",
    "# data dimensions \n",
    "print(\" Training data size :  \", train_data.shape)  # Number of samples * Length of the series * number of features\n",
    "print(\" Test data size :  \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "530fd845",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NTXent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m scarf_model\u001b[38;5;241m.\u001b[39mSCARF(\n\u001b[1;32m      5\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m      6\u001b[0m     emb_dim\u001b[38;5;241m=\u001b[39mrepr_dims,\n\u001b[1;32m      7\u001b[0m     corruption_rate\u001b[38;5;241m=\u001b[39mcorruptionRate,\n\u001b[1;32m      8\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m---> 10\u001b[0m ntxent_loss \u001b[38;5;241m=\u001b[39m \u001b[43mNTXent\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NTXent' is not defined"
     ]
    }
   ],
   "source": [
    "# Setting up the data laoder and initializing the models\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = scarf_model.SCARF(\n",
    "    input_dim=train_data.shape[1], \n",
    "    emb_dim=repr_dims,\n",
    "    corruption_rate=corruptionRate,\n",
    ").to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "ntxent_loss = NTXent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b8a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
