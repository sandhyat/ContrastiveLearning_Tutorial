{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7b5676",
   "metadata": {},
   "source": [
    "This file contains code to demonstrate how how contrastive learning is effectively an instance discrimination problem.\n",
    "In particular, we present a similarity rank based metric that can determine the difficulty of the instance discrimination task.\n",
    "\n",
    "We use openly available public dataset [Nomao](https://archive.ics.uci.edu/dataset/227/nomao) and are interested in learning contrastive loss based data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7406d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm, linear_model, model_selection, metrics, ensemble\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, pairwise,mutual_info_score,mean_squared_error\n",
    "from scipy import linalg, stats\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from matching.games import StableMarriage\n",
    "import pingouin as pg\n",
    "import datetime\n",
    "# from datetime import datetime\n",
    "import json, sys, argparse\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b3824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NTXentLoss(z1, z2,\n",
    "                 temperature=0.1):  # embeddings from known features of both databases followed by the unknown features\n",
    "    # compute the cosine similarity bu first normalizing and then matrix multiplying the known and unknown tensors\n",
    "    cos_sim_o = torch.div(torch.matmul(torch.nn.functional.normalize(z1),\n",
    "                                       torch.transpose(torch.nn.functional.normalize(z2), 0, 1)),\n",
    "                          temperature)\n",
    "\n",
    "    # for numerical stability  ## TODO update this logit name\n",
    "    logits_max_o, _ = torch.max(cos_sim_o, dim=1, keepdim=True)\n",
    "    logits_o = cos_sim_o - logits_max_o.detach()\n",
    "\n",
    "    # breakpoint()\n",
    "    if True:\n",
    "      # computing the exp logits\n",
    "      exp_o = torch.exp(logits_o)\n",
    "      batch_loss_o = - torch.log(exp_o.diag() / exp_o.sum(dim=0)).sum() - torch.log(\n",
    "        exp_o.diag() / exp_o.sum(dim=1)).sum()\n",
    "      # computing the avg rank of the positive examples for checking if the algo is learning the representation closer\n",
    "      # since we are computing the rank on the similarity so higher the better\n",
    "      avg_rank_cos_sim_o = np.trace(stats.rankdata(cos_sim_o.cpu().detach().numpy(), axis=1)) / len(cos_sim_o)\n",
    "\n",
    "    # print(\"This batch's loss and avg rank \", batch_loss_o.item(), batch_loss_r.item(), avg_rank_cos_sim_o, avg_rank_cos_sim_r)\n",
    "    return batch_loss_o, avg_rank_cos_sim_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7765ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder networks\n",
    "class MLP(torch.nn.Sequential):\n",
    "    \"\"\"Simple multi-layer perceptron with ReLu activation and optional dropout layer\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout=0.0):\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(torch.nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(torch.nn.Dropout(dropout))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        layers.append(torch.nn.Linear(in_dim, hidden_dim))\n",
    "\n",
    "        super().__init__(*layers)\n",
    "        \n",
    "class CL_model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        emb_dim,\n",
    "        encoder_depth=4,\n",
    "        head_depth=2,\n",
    "    ):\n",
    "        \"\"\"Implementation of a SimCLR kind basic CL approach.\n",
    "        It consists of an encoder that learns the embeddings.\n",
    "        It is done by minimizing the contrastive loss of a sample and an augmented view of it.\n",
    "            Args:\n",
    "                input_dim (int): size of the inputs\n",
    "                emb_dim (int): dimension of the embedding space\n",
    "                encoder_depth (int, optional): number of layers of the encoder MLP. Defaults to 4.\n",
    "                head_depth (int, optional): number of layers of the pretraining head. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.encoder = MLP(input_dim, emb_dim, encoder_depth)\n",
    "        self.pretraining_head = MLP(emb_dim, emb_dim, head_depth)\n",
    "\n",
    "        # initialize weights\n",
    "        self.encoder.apply(self._init_weights)\n",
    "        self.pretraining_head.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            module.bias.data.fill_(0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
